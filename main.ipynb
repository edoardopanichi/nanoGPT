{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial step is to import a dataset\n",
    "!curl -s -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))\n",
    "# we also want to see the first 100 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# our vocabulary is the set of unique characters in the text (in form of a list)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# we will create a mapping from characters to indices and vice versa. This is also known as tokenization.\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "# Then we make to lambda functions to convert strings to list of indices and vice versa\n",
    "encode = lambda x: [char_to_idx[ch] for ch in x]\n",
    "decode = lambda x: ''.join([idx_to_char[i] for i in x])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.LongTensor\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Next we will convert the dataset into a pytorch tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.type())\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we split the dataset into validation and training sets\n",
    "train_size = int(0.9 * len(data))\n",
    "val_data = data[train_size:]\n",
    "train_data = data[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the context is tensor([18]) the target is 47\n",
      "when the context is tensor([18, 47]) the target is 56\n",
      "when the context is tensor([18, 47, 56]) the target is 57\n",
      "when the context is tensor([18, 47, 56, 57]) the target is 58\n",
      "when the context is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "# We define the context window size, also called block size\n",
    "block_size = 8\n",
    "# Given a block size of 8, we can train the transformer to predict the second element in the sequence given the first element,\n",
    "# the third element given the first two, and so on. So actually, for a block size of 8, we will have 8 training examples from each sequence.\n",
    "# This is helps the network in two ways: it learns to predict the next element in a sequence no matter if the sequence is short or long,\n",
    "# and it makes training faster since we can train on multiple examples at the same time.\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1] # y_train will be the same as x_train, except shifted one position to the right\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # all elements up to t+1 (excluded)\n",
    "    target = y[t] # t-th target to predict, which corresponds to the t+1 of \"context\"\n",
    "    print(f\"when the context is {context} the target is {target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 8]), Target shape: torch.Size([4, 8])\n",
      "Inputs: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets: tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "-----------------\n",
      "\n",
      "Batch 1, when the context is tensor([24]) the target is 43\n",
      "Batch 1, when the context is tensor([24, 43]) the target is 58\n",
      "Batch 1, when the context is tensor([24, 43, 58]) the target is 5\n",
      "Batch 1, when the context is tensor([24, 43, 58,  5]) the target is 57\n",
      "Batch 1, when the context is tensor([24, 43, 58,  5, 57]) the target is 1\n",
      "Batch 1, when the context is tensor([24, 43, 58,  5, 57,  1]) the target is 46\n",
      "Batch 1, when the context is tensor([24, 43, 58,  5, 57,  1, 46]) the target is 43\n",
      "Batch 1, when the context is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is 39\n",
      "Batch 2, when the context is tensor([44]) the target is 53\n",
      "Batch 2, when the context is tensor([44, 53]) the target is 56\n",
      "Batch 2, when the context is tensor([44, 53, 56]) the target is 1\n",
      "Batch 2, when the context is tensor([44, 53, 56,  1]) the target is 58\n",
      "Batch 2, when the context is tensor([44, 53, 56,  1, 58]) the target is 46\n",
      "Batch 2, when the context is tensor([44, 53, 56,  1, 58, 46]) the target is 39\n",
      "Batch 2, when the context is tensor([44, 53, 56,  1, 58, 46, 39]) the target is 58\n",
      "Batch 2, when the context is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is 1\n",
      "Batch 3, when the context is tensor([52]) the target is 58\n",
      "Batch 3, when the context is tensor([52, 58]) the target is 1\n",
      "Batch 3, when the context is tensor([52, 58,  1]) the target is 58\n",
      "Batch 3, when the context is tensor([52, 58,  1, 58]) the target is 46\n",
      "Batch 3, when the context is tensor([52, 58,  1, 58, 46]) the target is 39\n",
      "Batch 3, when the context is tensor([52, 58,  1, 58, 46, 39]) the target is 58\n",
      "Batch 3, when the context is tensor([52, 58,  1, 58, 46, 39, 58]) the target is 1\n",
      "Batch 3, when the context is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is 46\n",
      "Batch 4, when the context is tensor([25]) the target is 17\n",
      "Batch 4, when the context is tensor([25, 17]) the target is 27\n",
      "Batch 4, when the context is tensor([25, 17, 27]) the target is 10\n",
      "Batch 4, when the context is tensor([25, 17, 27, 10]) the target is 0\n",
      "Batch 4, when the context is tensor([25, 17, 27, 10,  0]) the target is 21\n",
      "Batch 4, when the context is tensor([25, 17, 27, 10,  0, 21]) the target is 1\n",
      "Batch 4, when the context is tensor([25, 17, 27, 10,  0, 21,  1]) the target is 54\n",
      "Batch 4, when the context is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # number of independent sequences that are trained in parallel\n",
    "block_size = 8 # length of sequences/context size window\n",
    "\n",
    "# Now we make a function that generates a batch of training data or validation data based on the input\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    # We want to generate a batch of random sequences, to do this we will randomly sample a starting index for each sequence.\n",
    "    # This index has to be between 0 and the length of the dataset minus the block size.\n",
    "    ix = torch.randint(0, data.size(0) - block_size, (batch_size,))\n",
    "    # Each block is identified by an x and y (label) tensor\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # y will be the same as x, except shifted one position to the right\n",
    "    return x, y\n",
    "    \n",
    "x_example, y_example = get_batch('train')\n",
    "print(f\"Input shape: {x_example.shape}, Target shape: {y_example.shape}\")\n",
    "print(f\"Inputs: {x_example}\")\n",
    "print(f\"Targets: {y_example}\")\n",
    "print(\"\\n-----------------\\n\")\n",
    "\n",
    "# Now we make a loop that plots the input and the target for each sequence in the batch\n",
    "for batch in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = x_example[batch, :t+1]\n",
    "        target = y_example[batch, t]\n",
    "        print(f\"Batch {batch+1}, when the context is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a **large language model (LLM)**, **logits** are the raw, unnormalized output values produced by the model just before applying a **softmax function** to convert them into probabilities.\n",
    "\n",
    "We process a batch of token blocks in parallel using the `forward()` method. For each token in the blocks, the model predicts the probabilities of all possible tokens in the vocabulary, efficiently handling all sequences and tokens simultaneously. This enables the model to learn and make next-token predictions across the entire batch at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # for each token of the vocabulary, we will have a corresponding embedding. The right value for the embedding are learned \n",
    "        # during training. The torch method Embedding is used to create the embedding table. This is of size vocab_size x embedding_size, \n",
    "        # where embedding_size is a hyperparameter that we can choose, in this case we will use vocab_size.\n",
    "        # Remember that the embedding of a token is the starting point to make the predictions.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, target=None):\n",
    "        # idx: A tensor of shape (batch_size, block_size) containing token IDs.\n",
    "        # - batch_size: Number of sequences (blocks) processed in parallel.\n",
    "        # - block_size: Number of tokens in each sequence (i.e., the length of each block).\n",
    "        # Each element in idx represents a token in the vocabulary, serving as input to the model\n",
    "        # to retrieve embeddings and predict the probabilities of the next tokens.\n",
    "        logits = self.token_embedding_table(idx) # shape of logits: (batch_size, block_size, embedding_size)\n",
    "        \n",
    "        # NOTE: Currently, the logits are directly obtained from the embedding table without passing through additional model layers.\n",
    "        # This means the logits are simply the embeddings of the input tokens and do not include any further transformations.\n",
    "        \n",
    "        if target == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # To be compliant with the cross-entropy loss, we need to reshape the logits tensor to (batch_size * block_size, embedding_size).\n",
    "            # Remember: embedding_size == vocab_size\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            target = target.view(-1)\n",
    "            \n",
    "            # The loss tells us how well the model is doing on the training data. \n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_next_tokens):\n",
    "        \n",
    "        # NOTE: Since we are using a Bigram model, we will only consider the last token in the sequence to predict the next token.\n",
    "        # This function though, is constructed in a more general way, so in the future we can also exploit the other tokens in the sequence. \n",
    "        \n",
    "        # like in the forward method, idx is a tensor of shape (batch_size, block_size) containing token IDs.\n",
    "        for t in range(max_next_tokens):\n",
    "            # We get the predictions for the next token, i.e. the logits\n",
    "            logits, _ = self.forward(idx)\n",
    "            # But to predict the next token, we only need the last prediction, which is the one at the last position of the sequence.\n",
    "            last_logit = logits[:, -1, :] # shape: (batch_size, embedding_size)\n",
    "            # We get the token with the highest probability applying the softmax function to the last prediction\n",
    "            probabilities = F.softmax(last_logit, dim=-1) # shape: (batch_size, embedding_size)\n",
    "            # Sample from the probability distribution to get the token ID of the next token\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1) # shape: (batch_size, 1)\n",
    "            # We add the new token to the sequence\n",
    "            idx = torch.cat([idx, next_token], dim=1) # shape: (batch_size, block_size + 1)\n",
    "            \n",
    "        return idx\n",
    "            \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "# In PyTorch, every custom model class that inherits from nn.Module must define a forward method. This method specifies the \n",
    "# computation the model performs when it processes input data.\n",
    "# When you call model(x_example, y_example), PyTorch automatically invokes the forward method of the model.\n",
    "logits, loss = model(x_example, y_example)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# Now we make an example of how the model generates text. The input will be a zero, that is the first token in the vocabulary that \n",
    "# represent a \"start new line\" token. The batch size will be 1.\n",
    "input_sequence = torch.zeros(1, 1, dtype=torch.long)\n",
    "generated_sequence = model.generate(input_sequence, max_next_tokens=100)\n",
    "generated_sequence_first_batch = generated_sequence[0].tolist() # in this case we only have one batch\n",
    "generated_text = decode(generated_sequence_first_batch)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5728, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now we will train the model. \n",
    "# We will use the Adam W optimizer, which is a popular optimizer for training neural networks.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# The optimizer takes care of updating the model's parameters based on the computed gradients.\n",
    "\n",
    "# Training loop\n",
    "max_epochs = 10000\n",
    "batch_size = 32\n",
    "for epoch in range(max_epochs):\n",
    "    # Get a batch of training data\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    # Evaluate the model on the batch using the loss\n",
    "    _, loss = model(x_batch, y_batch)\n",
    "    # Reset the gradients to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Compute the gradients for the parameters\n",
    "    loss.backward()\n",
    "    # Update the parameters of the model\n",
    "    optimizer.step()\n",
    "\n",
    "# Print the loss after the last epoch\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y helti\n"
     ]
    }
   ],
   "source": [
    "# Now we generate text using the trained model\n",
    "input_sequence = torch.zeros(1, 1, dtype=torch.long)\n",
    "generated_sequence = model.generate(input_sequence, max_next_tokens=100)\n",
    "generated_sequence_first_batch = generated_sequence[0].tolist()\n",
    "generated_text = decode(generated_sequence_first_batch)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now here we will make a little piece of code to show how to implement a single-head perform attention mechanism\n",
    "torch.manual_seed(1337)\n",
    "batch_size,  block_size, embedding_size = 4, 8, 32\n",
    "# We will create a random tensor of shape (batch_size, block_size, embedding_size)\n",
    "x = torch.randn(batch_size, block_size, embedding_size)\n",
    "\n",
    "# Attention block\n",
    "head_size = 16\n",
    "key = nn.Linear(embedding_size, head_size, bias=False)\n",
    "query = nn.Linear(embedding_size, head_size, bias=False)\n",
    "# We apply the key and query linear layers to the input tensor. All the tokens (for a total of batch x block) in the input tensor are processed in parallel. All of them \n",
    "# will produce a key and a query. \n",
    "k = key(x) # shape: (batch_size, block_size, head_size)\n",
    "q = query(x) # shape: (batch_size, block_size, head_size)\n",
    "# We calculate the scaled dot product attention, the product between a query and a key indicates the affinity between the two tokens (to one the query was belonging, \n",
    "# to the other the key). These affinities are de facto the attention weights, that specify how much a token is important for another token.\n",
    "# N.B. for k we transpose the last two dimensions, because the first one is the batches, and elements in different batches are independent.\n",
    "weights = q @ k.transpose(-2, -1) # (batch_size, block_size, head_size) @ (batch_size, head_size, block_size) -> (batch_size, block_size, block_size)\n",
    "\n",
    "# Now we can make the matrix of all the weights in a computational efficient way, as explained in the iPad notes.\n",
    "tril = torch.tril(torch.ones(block_size, block_size))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "# The softmax function is applied to the weights to obtain the attention weights. The softmax function ensures that the attention weights sum to 1.\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "output = weights @ x # (batch_size, block_size, block_size) @ (batch_size, block_size, embedding_size) -> (batch_size, block_size, embedding_size)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
