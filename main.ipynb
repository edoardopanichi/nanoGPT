{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial step is to import a dataset\n",
    "!curl -s -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))\n",
    "# we also want to see the first 100 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# our vocabulary is the set of unique characters in the text (in form of a list)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# we will create a mapping from characters to indices and vice versa. This is also known as tokenization.\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "# Then we make to lambda functions to convert strings to list of indices and vice versa\n",
    "encode = lambda x: [char_to_idx[ch] for ch in x]\n",
    "decode = lambda x: ''.join([idx_to_char[i] for i in x])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.LongTensor\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Next we will convert the dataset into a pytorch tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.type())\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we split the dataset into validation and training sets\n",
    "train_size = int(0.9 * len(data))\n",
    "val_data = data[train_size:]\n",
    "train_data = data[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the context is tensor([18]) the target is 47\n",
      "when the context is tensor([18, 47]) the target is 56\n",
      "when the context is tensor([18, 47, 56]) the target is 57\n",
      "when the context is tensor([18, 47, 56, 57]) the target is 58\n",
      "when the context is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "# We define the context window size, also called block size\n",
    "block_size = 8\n",
    "# Given a block size of 8, we can train the transformer to predict the second element in the sequence given the first element,\n",
    "# the third element given the first two, and so on. So actually, for a block size of 8, we will have 8 training examples from each sequence.\n",
    "# This is helps the network in two ways: it learns to predict the next element in a sequence no matter if the sequence is short or long,\n",
    "# and it makes training faster since we can train on multiple examples at the same time.\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1] # y_train will be the same as x_train, except shifted one position to the right\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # all elements up to t+1 (excluded)\n",
    "    target = y[t] # t-th target to predict, which corresponds to the t+1 of \"context\"\n",
    "    print(f\"when the context is {context} the target is {target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 8]), Target shape: torch.Size([4, 8])\n",
      "Inputs: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets: tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "-----------------\n",
      "\n",
      "Batch 1, when the context is tensor([24]) the target is 43\n",
      "Batch 1, when the context is tensor([24, 43]) the target is 58\n",
      "Batch 1, when the context is tensor([24, 43, 58]) the target is 5\n",
      "Batch 1, when the context is tensor([24, 43, 58,  5]) the target is 57\n",
      "Batch 1, when the context is tensor([24, 43, 58,  5, 57]) the target is 1\n",
      "Batch 1, when the context is tensor([24, 43, 58,  5, 57,  1]) the target is 46\n",
      "Batch 1, when the context is tensor([24, 43, 58,  5, 57,  1, 46]) the target is 43\n",
      "Batch 1, when the context is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is 39\n",
      "Batch 2, when the context is tensor([44]) the target is 53\n",
      "Batch 2, when the context is tensor([44, 53]) the target is 56\n",
      "Batch 2, when the context is tensor([44, 53, 56]) the target is 1\n",
      "Batch 2, when the context is tensor([44, 53, 56,  1]) the target is 58\n",
      "Batch 2, when the context is tensor([44, 53, 56,  1, 58]) the target is 46\n",
      "Batch 2, when the context is tensor([44, 53, 56,  1, 58, 46]) the target is 39\n",
      "Batch 2, when the context is tensor([44, 53, 56,  1, 58, 46, 39]) the target is 58\n",
      "Batch 2, when the context is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is 1\n",
      "Batch 3, when the context is tensor([52]) the target is 58\n",
      "Batch 3, when the context is tensor([52, 58]) the target is 1\n",
      "Batch 3, when the context is tensor([52, 58,  1]) the target is 58\n",
      "Batch 3, when the context is tensor([52, 58,  1, 58]) the target is 46\n",
      "Batch 3, when the context is tensor([52, 58,  1, 58, 46]) the target is 39\n",
      "Batch 3, when the context is tensor([52, 58,  1, 58, 46, 39]) the target is 58\n",
      "Batch 3, when the context is tensor([52, 58,  1, 58, 46, 39, 58]) the target is 1\n",
      "Batch 3, when the context is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is 46\n",
      "Batch 4, when the context is tensor([25]) the target is 17\n",
      "Batch 4, when the context is tensor([25, 17]) the target is 27\n",
      "Batch 4, when the context is tensor([25, 17, 27]) the target is 10\n",
      "Batch 4, when the context is tensor([25, 17, 27, 10]) the target is 0\n",
      "Batch 4, when the context is tensor([25, 17, 27, 10,  0]) the target is 21\n",
      "Batch 4, when the context is tensor([25, 17, 27, 10,  0, 21]) the target is 1\n",
      "Batch 4, when the context is tensor([25, 17, 27, 10,  0, 21,  1]) the target is 54\n",
      "Batch 4, when the context is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "bath_size = 4 # number of independent sequences that are trained in parallel\n",
    "block_size = 8 # length of sequences/context size window\n",
    "\n",
    "# Now we make a function that generates a batch of training data or validation data based on the input\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    # We want to generate a batch of random sequences, to do this we will randomly sample a starting index for each sequence.\n",
    "    # This index has to be between 0 and the length of the dataset minus the block size.\n",
    "    ix = torch.randint(0, data.size(0) - block_size, (bath_size,))\n",
    "    # Each block is identified by an x and y (label) tensor\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # y will be the same as x, except shifted one position to the right\n",
    "    return x, y\n",
    "    \n",
    "x_example, y_example = get_batch('train')\n",
    "print(f\"Input shape: {x_example.shape}, Target shape: {y_example.shape}\")\n",
    "print(f\"Inputs: {x_example}\")\n",
    "print(f\"Targets: {y_example}\")\n",
    "print(\"\\n-----------------\\n\")\n",
    "\n",
    "# Now we make a loop that plots the input and the target for each sequence in the batch\n",
    "for batch in range(bath_size):\n",
    "    for t in range(block_size):\n",
    "        context = x_example[batch, :t+1]\n",
    "        target = y_example[batch, t]\n",
    "        print(f\"Batch {batch+1}, when the context is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a **large language model (LLM)**, **logits** are the raw, unnormalized output values produced by the model just before applying a **softmax function** to convert them into probabilities.\n",
    "\n",
    "We process a batch of token blocks in parallel using the `forward()` method. For each token in the blocks, the model predicts the probabilities of all possible tokens in the vocabulary, efficiently handling all sequences and tokens simultaneously. This enables the model to learn and make next-token predictions across the entire batch at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # for each token of the vocabulary, we will have a corresponding embedding. The right value for the embedding are learned \n",
    "        # during training. The torch method Embedding is used to create the embedding table. This is of size vocab_size x embedding_size, \n",
    "        # where embedding_size is a hyperparameter that we can choose, in this case we will use vocab_size.\n",
    "        # Remember that the embedding of a token is the starting point to make the predictions.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, target=None):\n",
    "        # idx: A tensor of shape (batch_size, block_size) containing token IDs.\n",
    "        # - batch_size: Number of sequences (blocks) processed in parallel.\n",
    "        # - block_size: Number of tokens in each sequence (i.e., the length of each block).\n",
    "        # Each element in idx represents a token in the vocabulary, serving as input to the model\n",
    "        # to retrieve embeddings and predict the probabilities of the next tokens.\n",
    "        logits = self.token_embedding_table(idx) # shape: (batch_size, block_size, embedding_size)\n",
    "        \n",
    "        # To be compliant with the cross-entropy loss, we need to reshape the logits tensor to (batch_size * block_size, embedding_size).\n",
    "        # Remember: embedding_size == vocab_size\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        # The loss tells us how well the model is doing on the training data. \n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "# In PyTorch, every custom model class that inherits from nn.Module must define a forward method. This method specifies the \n",
    "# computation the model performs when it processes input data.\n",
    "# When you call model(x_example, y_example), PyTorch automatically invokes the forward method of the model.\n",
    "logits, loss = model(x_example, y_example)\n",
    "print(logits.shape)\n",
    "print(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
